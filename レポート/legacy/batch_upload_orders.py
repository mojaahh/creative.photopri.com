#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å¤§é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã„ã‚ˆã†ã€å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã«åˆ†ã‘ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™
"""

import os
import sys
import csv
import logging
from datetime import datetime
from core.order_export import ShopifyOrderExporter
from core.spreadsheet_uploader import SpreadsheetUploader

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_upload.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BatchOrderUploader:
    """åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ã®æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.exporter = None
        self.uploader = None
        self.batch_size = 500  # 1å›ã‚ãŸã‚Šã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¡Œæ•°
    
    def setup_components(self):
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        try:
            logger.info("ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’é–‹å§‹")
            
            # æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–
            self.exporter = ShopifyOrderExporter()
            logger.info("âœ… æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã®åˆæœŸåŒ–å®Œäº†")
            
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
            self.uploader = SpreadsheetUploader()
            logger.info("âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã®åˆæœŸåŒ–å®Œäº†")
            
            return True
            
        except Exception as e:
            logger.error(f"ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def split_csv_file(self, csv_filepath: str) -> list:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        try:
            logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†å‰²ã‚’é–‹å§‹: {csv_filepath}")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            with open(csv_filepath, 'r', encoding='utf-8') as file:
                reader = csv.reader(file)
                all_data = list(reader)
            
            if not all_data:
                logger.warning("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return []
            
            header = all_data[0]
            data_rows = all_data[1:]
            total_rows = len(data_rows)
            
            logger.info(f"ç·è¡Œæ•°: {total_rows}è¡Œï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼é™¤ãï¼‰")
            
            # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            chunks = []
            for i in range(0, total_rows, self.batch_size):
                chunk = [header] + data_rows[i:i + self.batch_size]
                
                chunk_filename = f"chunk_{i//self.batch_size + 1:03d}_{len(chunk)-1}rows.csv"
                chunk_filepath = os.path.join('exports', chunk_filename)
                
                # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
                with open(chunk_filepath, 'w', newline='', encoding='utf-8') as chunk_file:
                    writer = csv.writer(chunk_file)
                    writer.writerows(chunk)
                
                chunks.append({
                    'filename': chunk_filename,
                    'filepath': chunk_filepath,
                    'data': chunk,
                    'row_count': len(chunk) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã
                })
                
                logger.info(f"ãƒãƒ£ãƒ³ã‚¯ {i//self.batch_size + 1}: {len(chunk)-1}è¡Œ")
            
            logger.info(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {len(chunks)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²å®Œäº†")
            return chunks
            
        except Exception as e:
            logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def upload_chunk_to_sheet(self, chunk: dict, sheet_name: str = "db", is_first_chunk: bool = False) -> bool:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        try:
            chunk_filename = chunk['filename']
            chunk_data = chunk['data']
            row_count = chunk['row_count']
            
            logger.info(f"ãƒãƒ£ãƒ³ã‚¯ '{chunk_filename}' ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹ ({row_count}è¡Œ)")
            
            if is_first_chunk:
                # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã¯é€šå¸¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ä»˜ãï¼‰
                if self.uploader.upload_data_to_sheet(sheet_name, chunk_data):
                    logger.info(f"âœ… ãƒãƒ£ãƒ³ã‚¯ '{chunk_filename}' ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ä»˜ãï¼‰")
                    return True
                else:
                    logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ '{chunk_filename}' ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
                    return False
            else:
                # 2ç•ªç›®ä»¥é™ã®ãƒãƒ£ãƒ³ã‚¯ã¯ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã§è¿½åŠ 
                data_without_header = chunk_data[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã
                if self.uploader.append_data_to_sheet(sheet_name, data_without_header):
                    logger.info(f"âœ… ãƒãƒ£ãƒ³ã‚¯ '{chunk_filename}' ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ˆãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼‰")
                    return True
                else:
                    logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ '{chunk_filename}' ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
                    return False
                
        except Exception as e:
            logger.error(f"ãƒãƒ£ãƒ³ã‚¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def batch_upload_orders(self, sheet_name: str = "db", force_initial: bool = True) -> bool:
        """æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        try:
            logger.info("åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’é–‹å§‹")
            
            # å…¨æœŸé–“ã®æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            logger.info("å…¨æœŸé–“ã®æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
            filename, is_initial = self.exporter.export_orders(force_initial=force_initial)
            
            if not filename:
                logger.error("æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            csv_filepath = f"exports/{filename}"
            logger.info(f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {filename}")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            chunks = self.split_csv_file(csv_filepath)
            
            if not chunks:
                logger.error("CSVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†å‰²ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®æº–å‚™
            if force_initial:
                # åˆå›å®Ÿè¡Œæ™‚ã¯ã‚·ãƒ¼ãƒˆã‚’ã‚¯ãƒªã‚¢
                logger.info(f"ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã®å†…å®¹ã‚’ã‚¯ãƒªã‚¢ä¸­...")
                self.uploader.clear_sheet_content(sheet_name)
                logger.info("âœ… ã‚·ãƒ¼ãƒˆã®ã‚¯ãƒªã‚¢å®Œäº†")
            
            # å„ãƒãƒ£ãƒ³ã‚¯ã‚’é †æ¬¡ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            successful_chunks = 0
            total_chunks = len(chunks)
            
            for i, chunk in enumerate(chunks, 1):
                logger.info(f"ãƒãƒ£ãƒ³ã‚¯ {i}/{total_chunks} ã®å‡¦ç†ä¸­...")
                
                # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã‹ã©ã†ã‹ã‚’åˆ¤å®š
                is_first_chunk = (i == 1)
                
                if self.upload_chunk_to_sheet(chunk, sheet_name, is_first_chunk):
                    successful_chunks += 1
                    logger.info(f"âœ… ãƒãƒ£ãƒ³ã‚¯ {i}/{total_chunks} å®Œäº†")
                else:
                    logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ {i}/{total_chunks} å¤±æ•—")
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶šè¡Œ
                
                # ãƒãƒ£ãƒ³ã‚¯é–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰
                if i < total_chunks:
                    logger.info("æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã¾ã§å¾…æ©Ÿä¸­...")
                    import time
                    time.sleep(2)
            
            # çµæœã‚µãƒãƒªãƒ¼
            logger.info("=" * 60)
            logger.info(f"åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {successful_chunks}/{total_chunks} ãƒãƒ£ãƒ³ã‚¯æˆåŠŸ")
            logger.info(f"å¯¾è±¡ã‚·ãƒ¼ãƒˆ: {sheet_name}")
            logger.info("=" * 60)
            
            return successful_chunks == total_chunks
            
        except Exception as e:
            logger.error(f"åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def cleanup_chunk_files(self):
        """åˆ†å‰²ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        try:
            exports_dir = "exports"
            if not os.path.exists(exports_dir):
                return
            
            # chunk_ã§å§‹ã¾ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            chunk_files = [f for f in os.listdir(exports_dir) if f.startswith('chunk_')]
            
            for chunk_file in chunk_files:
                filepath = os.path.join(exports_dir, chunk_file)
                try:
                    os.remove(filepath)
                    logger.info(f"ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤: {chunk_file}")
                except Exception as e:
                    logger.warning(f"ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ã«å¤±æ•—: {chunk_file} - {e}")
            
            logger.info(f"âœ… {len(chunk_files)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤å®Œäº†")
            
        except Exception as e:
            logger.error(f"ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        print("ğŸš€ æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™")
        
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®å‡¦ç†
        sheet_name = "db"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯dbã‚·ãƒ¼ãƒˆ
        force_initial = True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿
        
        if len(sys.argv) > 1:
            sheet_name = sys.argv[1]
        
        if len(sys.argv) > 2 and sys.argv[2].lower() == 'update':
            force_initial = False
            print("ğŸ”„ æ›´æ–°ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")
        else:
            print("ğŸ”„ å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")
        
        # åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
        batch_uploader = BatchOrderUploader()
        
        if not batch_uploader.setup_components():
            print("âŒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
        
        # åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
        success = batch_uploader.batch_upload_orders(sheet_name, force_initial)
        
        if success:
            print("\nğŸ‰ åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            print(f"ğŸ“Š å¯¾è±¡ã‚·ãƒ¼ãƒˆ: {sheet_name}")
            print(f"ğŸ”— ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURL: {batch_uploader.uploader.get_spreadsheet_url()}")
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            print("\nğŸ§¹ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...")
            batch_uploader.cleanup_chunk_files()
            
        else:
            print("\nâŒ åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            print("ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„: ãƒ¬ãƒãƒ¼ãƒˆ/batch_upload.log")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logger.error(f"ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
