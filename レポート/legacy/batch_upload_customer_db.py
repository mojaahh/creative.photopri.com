#!/usr/bin/env python3
"""
CustomerDBç”¨ã®åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å¤§ããªé¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å®‰å…¨ã«Google Sheetsã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™
"""

import os
import sys
import csv
import time
import logging
from typing import List, Dict
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from core.customer_db_generator import CustomerDBGenerator
from core.spreadsheet_uploader import SpreadsheetUploader

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_upload_customer_db.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CustomerDBBatchUploader:
    def __init__(self, batch_size: int = 500):
        """åˆæœŸåŒ–"""
        self.batch_size = batch_size
        self.generator = None
        self.uploader = None
        
    def setup_components(self):
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        try:
            self.generator = CustomerDBGenerator()
            self.uploader = SpreadsheetUploader()
            logger.info("âœ… ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            return False
    
    def split_csv_file(self, csv_filepath: str) -> List[Dict]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²"""
        try:
            chunks = []
            chunk_data = []
            chunk_number = 1
            
            with open(csv_filepath, 'r', encoding='utf-8-sig') as csvfile:
                reader = csv.reader(csvfile)
                header = next(reader)  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’å–å¾—
                
                for i, row in enumerate(reader):
                    chunk_data.append(row)
                    
                    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰ã€ã¾ãŸã¯æœ€å¾Œã®è¡Œã®å ´åˆ
                    if len(chunk_data) >= self.batch_size or i == len(list(csv.reader(open(csv_filepath, 'r', encoding='utf-8-sig')))) - 2:
                        # ãƒ˜ãƒƒãƒ€ãƒ¼ä»˜ãã§ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆ
                        chunk_with_header = [header] + chunk_data
                        
                        chunk_filename = f"customer_chunk_{chunk_number:03d}.csv"
                        chunk_filepath = f"exports/{chunk_filename}"
                        
                        # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                        with open(chunk_filepath, 'w', newline='', encoding='utf-8-sig') as chunk_file:
                            writer = csv.writer(chunk_file)
                            writer.writerows(chunk_with_header)
                        
                        chunks.append({
                            'filename': chunk_filename,
                            'filepath': chunk_filepath,
                            'data': chunk_with_header,
                            'row_count': len(chunk_with_header)
                        })
                        
                        logger.info(f"ãƒãƒ£ãƒ³ã‚¯ {chunk_number}: {len(chunk_data)}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜")
                        
                        chunk_data = []
                        chunk_number += 1
            
            logger.info(f"ğŸ“Š CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {len(chunks)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ")
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def cleanup_chunk_files(self, chunks: List[Dict]):
        """ä¸€æ™‚çš„ãªãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        try:
            for chunk in chunks:
                if os.path.exists(chunk['filepath']):
                    os.remove(chunk['filepath'])
                    logger.info(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {chunk['filename']}")
            logger.info("âœ… ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def upload_chunk_to_sheet(self, chunk: Dict, sheet_name: str = "CustomerDB", is_first_chunk: bool = False) -> bool:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        try:
            chunk_filename = chunk['filename']
            chunk_data = chunk['data']
            row_count = chunk['row_count']
            
            logger.info(f"ãƒãƒ£ãƒ³ã‚¯ '{chunk_filename}' ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹ ({row_count}è¡Œ)")
            
            if is_first_chunk:
                # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã¯é€šå¸¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ä»˜ãï¼‰
                if self.uploader.upload_data_to_sheet(sheet_name, chunk_data):
                    logger.info(f"âœ… ãƒãƒ£ãƒ³ã‚¯ '{chunk_filename}' ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ä»˜ãï¼‰")
                    return True
                else:
                    logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ '{chunk_filename}' ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
                    return False
            else:
                # 2ç•ªç›®ä»¥é™ã®ãƒãƒ£ãƒ³ã‚¯ã¯ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã§è¿½åŠ 
                data_without_header = chunk_data[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã
                if self.uploader.append_data_to_sheet(sheet_name, data_without_header):
                    logger.info(f"âœ… ãƒãƒ£ãƒ³ã‚¯ '{chunk_filename}' ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ˆãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼‰")
                    return True
                else:
                    logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ '{chunk_filename}' ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
                    return False
                    
        except Exception as e:
            logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def batch_upload_customer_db(self, sheet_name: str = "CustomerDB") -> bool:
        """é¡§å®¢ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        try:
            logger.info("ğŸš€ CustomerDBã®åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™")
            
            # 1. é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            logger.info("ğŸ“Š é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’é–‹å§‹...")
            if not self.generator.export_customers():
                logger.error("âŒ é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            # æœ€æ–°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            exports_dir = Path("exports")
            customer_files = list(exports_dir.glob("customers_export_*.csv"))
            if not customer_files:
                logger.error("âŒ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸé¡§å®¢ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            latest_file = max(customer_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"ğŸ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {latest_file.name}")
            
            # 2. CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²
            logger.info("âœ‚ï¸ CSVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†å‰²ã‚’é–‹å§‹...")
            chunks = self.split_csv_file(str(latest_file))
            if not chunks:
                logger.error("âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†å‰²ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            total_chunks = len(chunks)
            logger.info(f"ğŸ“Š åˆ†å‰²å®Œäº†: {total_chunks}å€‹ã®ãƒãƒ£ãƒ³ã‚¯")
            
            # 3. ã‚·ãƒ¼ãƒˆã‚’ã‚¯ãƒªã‚¢ã—ã¦æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            logger.info(f"ğŸ—‘ï¸ ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã®å†…å®¹ã‚’ã‚¯ãƒªã‚¢ä¸­...")
            if not self.uploader.clear_sheet(sheet_name):
                logger.error(f"âŒ ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã®ã‚¯ãƒªã‚¢ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            # 4. ãƒãƒ£ãƒ³ã‚¯ã‚’é †æ¬¡ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            successful_chunks = 0
            failed_chunks = 0
            
            for i, chunk in enumerate(chunks, 1):
                try:
                    logger.info(f"ğŸ“¤ ãƒãƒ£ãƒ³ã‚¯ {i}/{total_chunks} ã®å‡¦ç†ä¸­...")
                    
                    # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã‹ã©ã†ã‹ã‚’åˆ¤å®š
                    is_first_chunk = (i == 1)
                    
                    if self.upload_chunk_to_sheet(chunk, sheet_name, is_first_chunk):
                        successful_chunks += 1
                        logger.info(f"âœ… ãƒãƒ£ãƒ³ã‚¯ {i}/{total_chunks} å®Œäº†")
                    else:
                        failed_chunks += 1
                        logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ {i}/{total_chunks} å¤±æ•—")
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                    if i < total_chunks:
                        time.sleep(2)
                        
                except Exception as e:
                    failed_chunks += 1
                    logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ {i}/{total_chunks} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            # 5. ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            logger.info("ğŸ§¹ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...")
            self.cleanup_chunk_files(chunks)
            
            # 6. çµæœã‚’è¡¨ç¤º
            logger.info(f"ğŸ‰ åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            logger.info(f"æˆåŠŸ: {successful_chunks}/{total_chunks} ãƒãƒ£ãƒ³ã‚¯")
            if failed_chunks > 0:
                logger.warning(f"å¤±æ•—: {failed_chunks}/{total_chunks} ãƒãƒ£ãƒ³ã‚¯")
            
            return failed_chunks == 0
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="CustomerDBã®åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    parser.add_argument("--batch-size", type=int, default=500, help="ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 500ï¼‰")
    parser.add_argument("--sheet", default="CustomerDB", help="ã‚·ãƒ¼ãƒˆåï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: CustomerDBï¼‰")
    
    args = parser.parse_args()
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs("exports", exist_ok=True)
    
    # ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
    uploader = CustomerDBBatchUploader(batch_size=args.batch_size)
    
    # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    if not uploader.setup_components():
        logger.error("âŒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    # åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
    if uploader.batch_upload_customer_db(args.sheet):
        logger.info("ğŸ‰ CustomerDBã®åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        sys.exit(0)
    else:
        logger.error("âŒ CustomerDBã®åˆ†å‰²ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)

if __name__ == "__main__":
    main()
